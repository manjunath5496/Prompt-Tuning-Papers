# Prompt Tuning Papers

<ul>

                             

 <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(1).pdf" style="text-decoration:none;">Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(2).pdf" style="text-decoration:none;">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(3).pdf" style="text-decoration:none;">Few-Shot Bot: Prompt-Based Learning for Dialogue Systems</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(4).pdf" style="text-decoration:none;">Language Models are Few-Shot Learners</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(5).pdf" style="text-decoration:none;">Surface Form Competition:
Why the Highest Probability Answer Isn't Always Right</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(6).pdf" style="text-decoration:none;">KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(7).pdf" style="text-decoration:none;">Text Generation with Efficient (Soft) Q-Learning</a></li>

 <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(8).pdf" style="text-decoration:none;"> Multitask Prompted Training Enables
Zero-shot Task Generalization </a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(9).pdf" style="text-decoration:none;">Pre-Trained Models: Past, Present and Future</a></li>
  
   
 <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(10).pdf" style="text-decoration:none;">Learning To Prompt For Vision-language
Models</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(11).pdf" style="text-decoration:none;">Towards A Unified View Of
Parameter-efficient Transfer Learning</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(12).pdf" style="text-decoration:none;">Cross-Task Generalization
via Natural Language Crowdsourcing Instructions</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(13).pdf" style="text-decoration:none;">WARP: Word-level Adversarial ReProgramming</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(14).pdf" style="text-decoration:none;">GPT Understands, Too</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(15).pdf" style="text-decoration:none;">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(16).pdf" style="text-decoration:none;">Prompt-Learning for Fine-Grained Entity Typing</a></li>

  <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(17).pdf" style="text-decoration:none;">Noisy Channel Language Model Prompting
for Few-Shot Text Classification</a></li>   
  
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(18).pdf" style="text-decoration:none;">Making Pre-trained Language Models Better Few-shot Learners</a></li> 

  
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(19).pdf" style="text-decoration:none;">Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning</a></li> 

<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(20).pdf" style="text-decoration:none;">Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(21).pdf" style="text-decoration:none;">The Benefits and Costs of Writing a POSIX Kernel in a High-Level Language</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(22).pdf" style="text-decoration:none;">Finetuned Language Models
Are Zero-shot Learners</a></li> 
 <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(23).pdf" style="text-decoration:none;">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></li> 
 

   <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(24).pdf" style="text-decoration:none;">How Many Data Points is a PromptWorth?</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(25).pdf" style="text-decoration:none;">Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners</a></li>                              
 <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(26).pdf" style="text-decoration:none;">Why Do Pretrained Language Models Help in Downstream Tasks? An Analysis of Head and Prompt Tuning</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(27).pdf" style="text-decoration:none;">CPT: Colorful Prompt Tuning For Pre-trained Vision-language Models</a></li>
   
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(28).pdf" style="text-decoration:none;">Thinking Aloud:
Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(29).pdf" style="text-decoration:none;">A Good Prompt Is Worth Millions of Parameters? Low-resource Prompt-based Learning for Vision-Language Models</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(30).pdf" style="text-decoration:none;">CONTROL PREFIXES for Text Generation</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(31).pdf" style="text-decoration:none;">Factual Probing Is [MASK]: Learning vs. Learning to Recall</a></li> 
    <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(32).pdf" style="text-decoration:none;">True Few-Shot Learning with Language Models</a></li> 

   <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(33).pdf" style="text-decoration:none;">Paradigm Shift in Natural Language Processing</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(34).pdf" style="text-decoration:none;">SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis</a></li> 
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(35).pdf" style="text-decoration:none;">Constrained Language Models Yield Few-Shot Semantic Parsers</a></li> 

  <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(36).pdf" style="text-decoration:none;">PTR: Prompt Tuning with Rules for Text Classification</a></li> 
 
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(37).pdf" style="text-decoration:none;">The Power of Prompt Tuning for Low-Resource Semantic Parsing</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(38).pdf" style="text-decoration:none;">Parameter-Efficient Transfer Learning for NLP</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(39).pdf" style="text-decoration:none;">Calibrate Before Use:
Improving Few-Shot Performance of Language Models</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(40).pdf" style="text-decoration:none;">NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task —— Next Sentence Prediction</a></li>                              
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(41).pdf" style="text-decoration:none;">Do Prompt-Based Models Really Understand
the Meaning of their Prompts?</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(42).pdf" style="text-decoration:none;">AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(43).pdf" style="text-decoration:none;">Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models</a></li>
 <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(44).pdf" style="text-decoration:none;">Label Verbalization and Entailment
for Effective Zero- and Few-Shot Relation Extraction</a></li>
   <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(45).pdf" style="text-decoration:none;">Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning</a></li>  
   
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(46).pdf" style="text-decoration:none;">Revisiting Self-Training for Few-Shot Learning of Language Model</a></li> 
                             
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(47).pdf" style="text-decoration:none;">The Power of Scale for Parameter-Efficient Prompt Tuning</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(48).pdf" style="text-decoration:none;">PPT: Pre-trained Prompt Tuning for Few-shot Learning</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(49).pdf" style="text-decoration:none;">Improving and Simplifying Pattern Exploiting Training</a></li>
                              
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(50).pdf" style="text-decoration:none;">Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(51).pdf" style="text-decoration:none;">GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation</a></li>
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(52).pdf" style="text-decoration:none;">PADA: A Prompt-based Autoregressive Approach for Adaptation to Unseen Domains</a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(53).pdf" style="text-decoration:none;">Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference</a></li>
 
<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(54).pdf" style="text-decoration:none;">How Can We Know What Language Models Know? </a></li>

<li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(55).pdf" style="text-decoration:none;">It's Not Just Size That Matters:
Small Language Models Are Also Few-Shot Learners</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(56).pdf" style="text-decoration:none;">Knowledgeable Prompt-tuning:
Incorporating Knowledge into Prompt Verbalizer for Text Classification </a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(57).pdf" style="text-decoration:none;">MSP: Multi-Stage Prompting for Making
Pre-trained Language Models Better Translators</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(58).pdf" style="text-decoration:none;">Template-free Prompt Tuning for Few-shot NER</a></li>
    <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(59).pdf" style="text-decoration:none;">Learning How to Ask: Querying LMs with Mixtures of Soft Prompts</a></li>
 
  <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(60).pdf" style="text-decoration:none;">What Makes Good In-Context Examples for GPT-3? </a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(61).pdf" style="text-decoration:none;">Language Models as Knowledge Bases?</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(62).pdf" style="text-decoration:none;">Automatically IdentifyingWords That Can Serve as Labels for Few-Shot Text Classification</a></li>
 
   <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(63).pdf" style="text-decoration:none;">Prompt Tuning or Fine-Tuning - Investigating Relational Knowledge in Pre-Trained Language Models</a></li>                              

  <li><a target="_blank" href="https://github.com/manjunath5496/Prompt-Tuning-Papers/blob/master/p(64).pdf" style="text-decoration:none;">Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm</a></li>
 
   </ul>
     
     
     
